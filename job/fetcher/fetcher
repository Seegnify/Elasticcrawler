#!/bin/sh

SCRIPT=$(basename $0)
SCRIPT_OPTIONS=$@

# find crawler
EC_FETCH=$(which ec-fetch-urls)
if [ ! -f "$EC_FETCH" ]; then
  echo "Elasticcrawler not found on PATH"
  exit 1
fi

# load local settings
EC_BIN=$(dirname "$EC_FETCH")
EC_HOME=$(dirname "$EC_BIN")
EC_LIB="$EC_HOME/lib"
CONFIG="$EC_HOME/conf/elasticcrawler.conf"
if [ -f "$CONFIG" ]; then
  . "$CONFIG"
else
  echo "Missing settings file: '$CONFIG'"
  exit 2
fi

# load job settings
CONFIG="$PWD/$SCRIPT.conf"
if [ -f "$CONFIG" ]; then
  . "$CONFIG"
else
  echo "Missing settings file: '$CONFIG'"
  exit 3
fi

# setup logging
if [ ! -d "$LOGS_DIRECTORY" ]; then
  echo "Logs directory '$LOGS_DIRECTORY' is missing" >&2
  exit 5
fi
EC_LOG="$LOGS_DIRECTORY/$SCRIPT-$JOB_DATE_TIME-$PPID.log"
date >> "$EC_LOG"
if [ $? != 0 ]; then
  echo "Unable to access log file: '$EC_LOG'" >&2
  exit 6
fi

# log start
echo "Logging to $EC_LOG"
echo "Host name/address: $SHC_HOST" >> "$EC_LOG"
echo "Starting: $SCRIPT $SCRIPT_OPTIONS" >> "$EC_LOG"

# validate input arguments
if [ $# != 1 ]; then
cat << EOF
Syntax: $SCRIPT <SET_SIZE>

  Fetch urls defined by ES scroll ID in scroll.id file.

Options:

  SET_SIZE - Number of urls to fetch by one job.
EOF
  exit 7
fi

# get input params
SET_SIZE=$1

# cleanup on singal
trap onsignal HUP INT TERM

# signal handler
onsignal() {
  cleanup
  exit 4
}

# cleanup routine
cleanup() {
  rm -rf $TMPDIR
}

# setup work folder
TMPDIR=$(mktemp -d --tmpdir ec-XXXXXXXX)
if [ ! -d "$TMPDIR" ]; then
  exit 8
fi

# get scroll id from file
ES_SCROLL="$PWD/scroll.id"
SCROLL_ID=$(cat "$ES_SCROLL")

# count fetched urls
FETCHED_URLS=0

# collect the url set by scrolling
while [ 1 ]
do
  # get urls at current scroll ID and increment ID
  "$EC_BIN/ec-list-urls" -i "$ES_SCROLL" > scroll; RET=$?
  if [ $RET != 0 ]; then
    echo "Failed to list urls at scroll ID:" $(cat "$ES_SCROLL") "($RET)"
    exit 9
  fi

  cat scroll >> "$TMPDIR/urls"
  SCROLL_URLS=$(cat scroll | wc -l)
  FETCHED_URLS=$((FETCHED_URLS+SCROLL_URLS))

  # if all urls have been fetched, stop fetching
  if [ $SCROLL_URLS = 0 ]; then
    break
  fi

  # if count of fetched urls is >= set size, create another set elsewhere
  if [ $FETCHED_URLS -ge $SET_SIZE ]; then
    shc start -r ec-fetcher "$PWD" "$SCRIPT" $SET_SIZE
    break
  fi
done >> "$EC_LOG" 2>&1

# report number of fetched urls
echo "Retrieved: $FETCHED_URLS urls" >> "$EC_LOG"

# fetch the url set
shuf "$TMPDIR/urls" > "$TMPDIR/urls.shuf"
rm "$TMPDIR/urls"
"$EC_FETCH" "$TMPDIR/urls.shuf" >> "$EC_LOG" 2>&1

# cleanup work dir
cleanup >> "$EC_LOG" 2>&1

# log stop
echo "Stopping: $SCRIPT" >> "$EC_LOG"
date
