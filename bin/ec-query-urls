#!/bin/sh

# load settings
EC_BIN=$(dirname "$0")
EC_HOME=$(dirname "$EC_BIN")
EC_LIB="$EC_HOME/lib"
CONFIG="$EC_HOME/conf/elasticcrawler.conf"
if [ -f "$CONFIG" ]; then
  . "$CONFIG"
else
  echo "Missing settings file: '$CONFIG'"
  exit 1
fi
export PYTHONPATH="$EC_LIB"

syntax() {
cat <<EOF
Syntax: $0 {<FILE>|-} [-p] [-n] [-h] [-r]

Options:

  FILE - file to read the urls from (one url per line).
  '-'  - read urls from standard input (one url per line)."
  -p   - show type 'page'.
  -n   - show type 'node'.
  -h   - show type 'host'.
  -r   - show type 'rank'.
EOF
}

# validate input arguments
if [ $# -lt 1 ]; then
  syntax
  exit 3
fi

# assign input arguments
if [ "$1" = "-" ]; then
  INPUT="/dev/stdin"
else
  INPUT=$1
fi
shift 1

# read input arguments
while getopts pnhr opt
do
  case $opt in
    p)  PAGE="on";;
    n)  NODE="on";;
    h)  HOST="on";;
    r)  RANK="on";;
    *)  syntax
        exit 4
        ;;
  esac
done

# get network location id
get_netloc_id() {
local URL=$(echo $URL | sed "s/'/\\\'/g")
python -c "
import elasticcrawler as ec
print ec.get_netloc_id('$URL')
"
}

# read urls from file or stdin
while read URL
do

  URL_ID=$(echo -n $URL | sha1sum | cut -c1-40)
  HOST_ID=$(get_netloc_id "$URL")

  # query page
  if [ "$PAGE" = "on" ]; then
    curl -sS -XGET "$ES_HOST:$ES_PORT/$ES_INDEX/page/$URL_ID" | jq .
  fi

  # query host
  if [ "$HOST" = "on" ]; then
    curl -sS -XGET "$ES_HOST:$ES_PORT/$ES_INDEX/host/$HOST_ID" | jq .
  fi

  # query node (format date)
  if [ "$NODE" = "on" ]; then
    REQUEST="$ES_HOST:$ES_PORT/$ES_INDEX/node/$URL_ID?fields=_source,_timestamp"
    RESPONSE=$(curl -sS -XGET "$REQUEST")
    TIMESTAMP=$(echo $RESPONSE | jq -e .fields._timestamp)
    if [ $? = 0 ]; then
      TIMESTAMP=$(date -d@$((TIMESTAMP/1000)))
      echo $RESPONSE | jq .fields._timestamp=\""$TIMESTAMP"\"
    else
      echo $RESPONSE | jq .
    fi
  fi

  # query rank
  if [ "$RANK" = "on" ]; then
    curl -sS -XGET "$ES_HOST:$ES_PORT/$ES_INDEX/rank/$URL_ID" | jq .
  fi

done < "$INPUT"

